{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e1a349-479f-4a70-9f22-ce2b31000e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import DataParser\n",
    "# from joblib.numpy_pickle_utils import xrange\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.preprocessing        import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection      import train_test_split\n",
    "from tensorflow.keras             import Sequential\n",
    "from tensorflow.keras.layers      import Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.utils       import to_categorical\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6646ae3-6fdc-48d2-aa17-5d5b90baf64c",
   "metadata": {},
   "source": [
    "# ANN Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414ac86",
   "metadata": {},
   "source": [
    "get the dictionary of names and categories, one hot encode each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537e6f51-797c-4594-a175-eb36db3c8bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mDataParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# this function works\u001b[39;00m\n\u001b[1;32m      3\u001b[0m names      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      4\u001b[0m categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_dict\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/mnt/d/Classes/crown102/POI-Categorization-RNN/DataParser.py:79\u001b[0m, in \u001b[0;36mget_input_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m db \u001b[38;5;241m=\u001b[39m getDB()\n\u001b[1;32m     78\u001b[0m train_db \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 79\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[43mgetHeaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Ensure the indices are correct\u001b[39;00m\n\u001b[1;32m     82\u001b[0m category_key \u001b[38;5;241m=\u001b[39m head[\u001b[38;5;241m12\u001b[39m]  \u001b[38;5;66;03m# Replace with the correct header name if needed\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Classes/crown102/POI-Categorization-RNN/DataParser.py:22\u001b[0m, in \u001b[0;36mgetHeaders\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetHeaders\u001b[39m():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# returns a list of all headers of the json list\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mgetDB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(db[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m headers\n",
      "File \u001b[0;32m/mnt/d/Classes/crown102/POI-Categorization-RNN/DataParser.py:11\u001b[0m, in \u001b[0;36mgetDB\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m db \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pathToFile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m db_file:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m db_file:\n\u001b[1;32m     12\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dict = DataParser.get_input_data() # this function works\n",
    "\n",
    "names      = list(data_dict.keys())\n",
    "categories = list(data_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d808c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unq_categories \u001b[38;5;241m=\u001b[39m \u001b[43mDataParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_unique_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unq_categories)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m distinct categories\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# → should be ~1311\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Classes/crown102/POI-Categorization-RNN/DataParser.py:51\u001b[0m, in \u001b[0;36mget_unique_categories\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m cat_data:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (category \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eachCat):\n\u001b[0;32m---> 51\u001b[0m         \u001b[43meachCat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m unique \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(entry):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unq_categories = DataParser.get_unique_categories()\n",
    "print(f\"Found {len(unq_categories)} distinct categories\")  # → should be ~1311\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cats = sorted(unq_categories)\n",
    "mlb = MultiLabelBinarizer(classes=sorted_cats)\n",
    "\n",
    "def split_cats(entry):\n",
    "    if not isinstance(entry, str):\n",
    "        return []\n",
    "    return [cat.strip() for cat in entry.split(',') if cat.strip()]\n",
    "\n",
    "Y = mlb.fit_transform([ split_cats(e) for e in categories ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(\n",
    "#     analyzer='char_wb',    # good for short text\n",
    "#     ngram_range=(2,4),\n",
    "#     max_features=5000\n",
    "# )\n",
    "\n",
    "# X = vectorizer.fit_transform(names)  # sparse matrix (150k × 5000)         # scipy sparse matrix, shape = (150k, 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502c4fa",
   "metadata": {},
   "source": [
    "New Transformer:\n",
    "instead of TF-IDF, I'm using Sentence Transformers (SBERT) to vectorize the names. Ideally, should work better than ngrams\n",
    "\"all-MiniLM-L6-v2” is small & fast with 384-dim embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')    \n",
    "X = embedder.encode(names, show_progress_bar=True, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfb07b-66ad-4870-90af-415b7b057940",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9523b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    InputLayer(input_shape=(X_train.shape[1],), sparse=True),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64,  activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(Y.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "# model = load_model(\"business_ann_model.h5\")\n",
    "\n",
    "# with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "#     vectorizer = pickle.load(f)\n",
    "\n",
    "# with open(\"mlb.pkl\", \"rb\") as f:\n",
    "#     mlb = pickle.load(f)\n",
    "    \n",
    "# sorted_cats = mlb.classes_.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f64d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_sparse(csr):\n",
    "    coo     = csr.tocoo()\n",
    "    indices = np.vstack((coo.row, coo.col)).T\n",
    "    st      = tf.sparse.SparseTensor(indices, coo.data.astype(np.float32), coo.shape)\n",
    "    return tf.sparse.reorder(st)\n",
    "\n",
    "def predict_business(name, threshold=0.5):\n",
    "    # 1) CSR → SparseTensor\n",
    "    v_sp   = to_tf_sparse(vectorizer.transform([name]))\n",
    "    # 2) Predict on sparse\n",
    "    probs  = model.predict(v_sp)[0]\n",
    "    # 3) Threshold\n",
    "    hits   = [cat for cat, p in zip(sorted_cats, probs) if p >= threshold]\n",
    "    return hits, probs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f708c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub, X_val, Y_train_sub, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# turn them into plain NumPy arrays\n",
    "X_train_sub_sp = to_tf_sparse(X_train_sub)\n",
    "X_val_sp       = to_tf_sparse(X_val)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_sub_sp,    # <-- sparse tensor here\n",
    "    Y_train_sub,\n",
    "    validation_data=(X_val_sp, Y_val),  # <-- sparse tensor here\n",
    "    epochs=10,\n",
    "    batch_size=256\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d91b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary cross-entropy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sp = to_tf_sparse(X_test)\n",
    "loss, acc = model.evaluate(X_test_sp, Y_test, verbose=0)\n",
    "print(f\"Test accuracy: {acc:.3f}\")\n",
    "\n",
    "model.save(\"business_ann_model.h5\")\n",
    "\n",
    "\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open(\"mlb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlb, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63b0b0",
   "metadata": {},
   "source": [
    "Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdd1e5",
   "metadata": {},
   "source": [
    "This model is able to predict the following really well\n",
    "- Chain Restaurants, Pizza, or Food (In generate)\n",
    "- Home Improvement Stores (Home Depot, Ace Hardware, ~Best Buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"business_ann_model.h5\")\n",
    "\n",
    "with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "with open(\"mlb.pkl\", \"rb\") as f:\n",
    "    mlb = pickle.load(f)\n",
    "\n",
    "sorted_cats = mlb.classes_.tolist()   # the same order you trained with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ded0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats, probs = predict_business(\"Verve Coffee Roasters\", threshold=0.5)\n",
    "print(\"Predicted categories:\", cats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
